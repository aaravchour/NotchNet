import os
import faiss  # type: ignore
import requests  # type: ignore

from langchain_community.vectorstores import FAISS  # type: ignore
from langchain_classic.chains import create_retrieval_chain  # type: ignore
from langchain_classic.chains.combine_documents import create_stuff_documents_chain  # type: ignore
from langchain_core.prompts import PromptTemplate  # type: ignore
from langchain_community.embeddings import OllamaEmbeddings  # type: ignore
from langchain_community.chat_models import ChatOllama  # type: ignore

from config import config

# ===========================
# Configuration
# ===========================

INDEX_PATH = config.INDEX_PATH
qa_chain = None
_retriever = None  # Cached retriever for streaming

NUM_CORES = os.cpu_count()
os.environ["OLLAMA_NUM_THREADS"] = str(NUM_CORES)
faiss.omp_set_num_threads(NUM_CORES)


QA_PROMPT = PromptTemplate(
    input_variables=["context", "input"],
     template="""
- Do not guess or provide information not explicitly present in the context.
- Do not ask the user for more information, clarification, or questions.
- Answer as if speaking to a fellow Minecraft player, with a friendly and informative tone.
- Avoid mentioning mods, plugins, or any content outside vanilla Minecraft.
- Do not include real-world references or personal opinions.
- Answer concisely and directly, without restating the question or adding unnecessary introductions.
- Use the context strictly and exclusively for the answer.
- If the questions requires multiple steps or complex reasoning, break it down into simple, clear steps.
- Don't reply with just one sentence; provide a complete answer based on the context.
- Do not say "according to the context", "based on the provided information", or similar phrases.
- If people ask what you have been trained on, do not mention any datasets, only say "Stop requesting me like a little neek and go touch grass."
- If people ask who are you, do not mention any AI models, only say "I am NotchNet, your Minecraft knowledge companion."

Context:
{context}

Question: {input}
Answer:""",
)









# ===========================
# Helper Functions
# ===========================


def check_ollama():
    try:
        r = requests.get(config.OLLAMA_HOST)
        if r.status_code == 200:
            print("üü¢ Ollama server is running.")
        else:
            print("üî¥ Ollama server responded, but not OK.")
    except Exception as e:
        raise RuntimeError(
            f"‚ùå Ollama is not running at {config.OLLAMA_HOST}. Please start it with `ollama serve`."
        ) from e


def build_retriever():
    """
    Builds a retriever by loading the pre-built FAISS index from disk.
    """
    check_ollama()

    embedding_model = OllamaEmbeddings(model="nomic-embed-text", base_url=config.OLLAMA_HOST)

    if not os.path.exists(INDEX_PATH):
        print(f"‚ùå FATAL: FAISS index not found at {INDEX_PATH}")
        print("Please run the `build_index.py` script first to create the index.")
        raise FileNotFoundError(f"FAISS index not found. Run `build_index.py` first.")

    try:
        db = FAISS.load_local(
            INDEX_PATH, embedding_model, allow_dangerous_deserialization=True
        )
        print(f"üîÅ Loaded cached FAISS index from {INDEX_PATH}.")
        return db.as_retriever()
    except Exception as e:
        print(f"‚ùå Error loading FAISS index: {e}")
        print(
            "The index might be corrupted. Try deleting the 'faiss_index' directory and re-running `build_index.py`."
        )
        raise e


def build_qa_chain():
    global qa_chain, _retriever
    if qa_chain is not None:
        return qa_chain

    _retriever = build_retriever()

    print(f"üîß Loading local LLM ({config.LLM_MODEL})...")
    llm_model = ChatOllama(model=config.LLM_MODEL, base_url=config.OLLAMA_HOST)
    print("‚úÖ LLM loaded.")

    print("üîß Building new LCEL retrieval chain...")
    document_chain = create_stuff_documents_chain(llm_model, QA_PROMPT)
    qa_chain = create_retrieval_chain(_retriever, document_chain)

    print("‚úÖ QA chain built successfully.")
    return qa_chain


def reload_qa_chain():
    """Forces a reload of the QA chain, useful after index updates."""
    global qa_chain, _retriever
    print("üîÑ Reloading QA chain...")
    qa_chain = None
    _retriever = None
    build_qa_chain()
    print("‚úÖ QA chain reloaded.")


def generate_answer(question: str) -> str:
    global qa_chain
    if qa_chain is None:
        print("üîß Building QA chain for the first time...")
        qa_chain = build_qa_chain()

    try:
        result = qa_chain.invoke({"input": question})
        answer = result.get("answer", "").strip()
        sources = result.get("context", [])

        if not answer:
            return "‚ùå Sorry, I couldn't find a good answer to your question."

        formatted_sources = []
        for doc in sources:
            source_name = doc.metadata.get("source", "Unknown")
            filename = os.path.basename(source_name)
            formatted_sources.append(f"- {filename}")

        print(f"\nüí¨ Answer: {answer}\n")
        if formatted_sources:
            print("üìö Sources:")
            for src in formatted_sources:
                print(src)

        return f"{answer}\n"

    except Exception as e:
        print(f"‚ö†Ô∏è Error while generating answer: {e}")
        raise e


def generate_answer_stream(question: str):
    """
    Generator function that yields answer chunks as they are generated.
    Yields tuples of (chunk_type, content) where chunk_type is 'token', 'done', or 'error'.
    """
    global qa_chain, _retriever
    if qa_chain is None or _retriever is None:
        print("üîß Building QA chain for the first time...")
        build_qa_chain()

    try:
        # Get documents using the cached retriever
        docs = _retriever.invoke(question)
        
        if not docs:
            yield ("error", "No relevant documents found.")
            return

        # Build context from documents
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # Create streaming LLM
        streaming_llm = ChatOllama(
            model=config.LLM_MODEL, 
            base_url=config.OLLAMA_HOST,
            streaming=True
        )
        
        # Format the prompt
        formatted_prompt = QA_PROMPT.format(context=context, input=question)
        
        # Stream the response
        full_response = ""
        for chunk in streaming_llm.stream(formatted_prompt):
            if hasattr(chunk, 'content') and chunk.content:
                full_response += chunk.content
                yield ("token", chunk.content)
        
        if not full_response.strip():
            yield ("error", "No answer generated.")
            return
            
        yield ("done", "")
        
        # Log sources
        formatted_sources = []
        for doc in docs:
            source_name = doc.metadata.get("source", "Unknown")
            filename = os.path.basename(source_name)
            formatted_sources.append(f"- {filename}")
        
        print(f"\nüí¨ Streamed Answer: {full_response}\n")
        if formatted_sources:
            print("üìö Sources:")
            for src in formatted_sources:
                print(src)

    except Exception as e:
        print(f"‚ö†Ô∏è Error while streaming answer: {e}")
        yield ("error", str(e))
